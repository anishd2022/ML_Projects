x_2 <- vball_data$spike
X <- as.matrix(cbind(x_0, x_1, x_2))
y <- as.numeric(vball_data$is_Libero)
logistic_loss <- function(w) {
}
c(0, 5, 6) %*% X
t(c(0, 5, 6)) %*% X
X
dim(X)
dim(c(0, 4, 5))
dim(as.matrix(c(0, 5, 6)))
t(as.matrix(c(0, 5, 6))) %*% X
dim(t(as.matrix(c(0, 5, 6))))
c(2, 3, 4) %*% X[1, ]
# predict if player is libero or not based on height and spike score:
# create design matrix
x_0 <- rep(1, nrow(vball_data))
x_1 <- vball_data$height
x_2 <- vball_data$spike
X <- as.matrix(cbind(x_0, x_1, x_2))
# set y to be 0 or 1 based on Libero
y <- as.numeric(vball_data$is_Libero)
n <- nrow(vball_data)
# create cross entropy loss function for logistic regression
sigmoid <- function(x) {
1 / (1 + exp(-x))
}
logistic_loss <- function(w) {
summation_term <- c()
for (i in 1:n) {
summation <- y[i] * log(sigmoid(w %*% X[i, ])) +
(1 - y[i]) * log(1 - sigmoid(w %*% X[i, ]))
summation_term <- append(summation_term, summation)
}
-(1/n) * sum(summation_term)
}
gradient <- function(w) {
summation_term <- c()
for (i in 1:n) {
summation <- (y[i] - sigmoid(w %*% X[i, ])) * (X[i, ])
summation_term <- append(summation_term, summation)
}
-(1/n) * sum(summation_term)
}
# implementing normalized gradient descent:
grad_desc_norm <- function(g, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n + 1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradientˆ2))
result
}
# iterate for K iterations:
for (i in 1:K) {
# calculate gradient at current step:
grad_curr <- gradient(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - (alpha / norm) * grad_curr
# update output matrix
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# implementing normalized gradient descent:
grad_desc_norm <- function(g, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n + 1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradientˆ2))
result
}
# iterate for K iterations:
for (i in 1:K) {
# calculate gradient at current step:
grad_curr <- gradient(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - (alpha / norm) * grad_curr
# update output matrix
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# running normalized gradient descent:
grad_desc_norm(logistic_loss, 0.001, c(25, 0, 0), 10000)
# implementing normalized gradient descent:
grad_desc_norm <- function(g, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n + 1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradientˆ2))
result
}
# iterate for K iterations:
for (i in 1:K) {
# calculate gradient at current step:
grad_curr <- gradient(curr_point)
print(grad_curr)
break
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - (alpha / norm) * grad_curr
# update output matrix
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# running normalized gradient descent:
grad_desc_norm(logistic_loss, 0.001, c(25, 0, 0), 10000)
# predict if player is libero or not based on height and spike score:
# create design matrix
x_0 <- rep(1, nrow(vball_data))
x_1 <- vball_data$height
x_2 <- vball_data$spike
X <- as.matrix(cbind(x_0, x_1, x_2))
# set y to be 0 or 1 based on Libero
y <- as.numeric(vball_data$is_Libero)
n <- nrow(vball_data)
# create cross entropy loss function for logistic regression
sigmoid <- function(x) {
1 / (1 + exp(-x))
}
logistic_loss <- function(w) {
summation_term <- c()
for (i in 1:n) {
summation <- y[i] * log(sigmoid(w %*% X[i, ])) +
(1 - y[i]) * log(1 - sigmoid(w %*% X[i, ]))
summation_term <- append(summation_term, summation)
}
-(1/n) * sum(summation_term)
}
gradient <- function(w) {
summation_term <- c()
for (i in 1:n) {
summation <- (y[i] - sigmoid(w %*% X[i, ])) %*% (X[i, ])
summation_term <- append(summation_term, summation)
}
-(1/n) * sum(summation_term)
}
# implementing normalized gradient descent:
grad_desc_norm <- function(g, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n + 1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradientˆ2))
result
}
# iterate for K iterations:
for (i in 1:K) {
# calculate gradient at current step:
grad_curr <- gradient(curr_point)
print(grad_curr)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - (alpha / norm) * grad_curr
# update output matrix
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# running normalized gradient descent:
grad_desc_norm(logistic_loss, 0.001, c(25, 0, 0), 10000)
# implementing normalized gradient descent:
grad_desc_norm <- function(g, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n + 1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations:
for (i in 1:K) {
# calculate gradient at current step:
grad_curr <- gradient(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - (alpha / norm) * grad_curr
# update output matrix
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# running normalized gradient descent:
grad_desc_norm(logistic_loss, 0.001, c(25, 0, 0), 10000)
summary(glm(y ~ X[ , -1], family = "binomial"))
# g is a function: Rˆn -> R
# grad_g is the gradient of g: a function ∇g: Rˆn -> Rˆn
# hess_g is the hessian of g: a function ∇g: Rˆn -> Rˆ(nxn)
# w0 is the initial point
# K is the number of iterations
newts_method <- function(g, grad_g, hess_g, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# iterate for K iterations
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate hessian at current step
hess_curr <- hess_g(curr_point)
# calculate inverse of the hessian
inv_hess <- solve(hess_curr)
# update step
mat[i, 1:length(w0)] <- curr_point - inv_hess %*% grad_curr
curr_point <- mat[i, 1:length(w0)]
# fill out function output given current point
mat[i, length(w0) + 1] <- g(curr_point)
}
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
# check if there are duplicate minimum values
min_indices <- which(outputs == min_val)
num_min_indices <- length(min_indices)
if (num_min_indices > 1) {
# if there are multiple min values, choose the first one
min_index <- inputs[min_indices[1], ]
} else {
# If there is only one unique minimum value, directly extract its index
min_index <- inputs[min_indices, ]
}
result$index <- min_index
result$val <- min_val
# return result
return(result)
}
# g is a function: Rˆn -> R
# grad_g is the gradient of g: a function ∇g: Rˆn -> Rˆn
# hess_g is the hessian of g: a function ∇g: Rˆn -> Rˆ(nxn)
# w0 is the initial point
# K is the number of iterations
newts_method <- function(g, grad_g, hess_g, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# iterate for K iterations
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate hessian at current step
hess_curr <- hess_g(curr_point)
# calculate inverse of the hessian
inv_hess <- solve(hess_curr)
# update step
mat[i, 1:length(w0)] <- curr_point - inv_hess %*% grad_curr
curr_point <- mat[i, 1:length(w0)]
# fill out function output given current point
mat[i, length(w0) + 1] <- g(curr_point)
}
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
# check if there are duplicate minimum values
min_indices <- which(outputs == min_val)
num_min_indices <- length(min_indices)
if (num_min_indices > 1) {
# if there are multiple min values, choose the first one
min_index <- inputs[min_indices[1], ]
} else {
# If there is only one unique minimum value, directly extract its index
min_index <- inputs[min_indices, ]
}
result$index <- min_index
result$val <- min_val
# return result
return(result)
}
# implementing ridge loss function:
n <- nrow(vball_data)
y <- vball_data$spike
ridge_loss_function <- function(w) {
(1/n) * t(y - design_matrix %*% w) %*% (y - design_matrix %*% w) + 1*t(w)%*%w
}
dim(t(design_matrix)%*%design_matrix)
dim(t(design_matrix)%*%design_matrix)[1]
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*n*I) %*% (t(design_matrix) %*% y)
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*n*I) %*% (t(design_matrix) %*% y)
w_prime
standardize_vector <- function(vector) {
mean_value <- mean(vector)
sd_value <- sd(vector)
standardized_values <- (vector - mean_value) / sd_value
}
intercept <- rep(1, nrow(vball_data))
height <- standardize_vector(vball_data$height)
weight <- standardize_vector(vball_data$weight)
block <- standardize_vector(vball_data$block)
design_matrix <- as.matrix(cbind(intercept, height, weight, block))
head(design_matrix)
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*n*I) %*% (t(design_matrix) %*% y)
w_prime
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*n*I) %*% (t(design_matrix) %*% y)
w_prime
install.packages("glmnet")
library(glmnet)
install.packages("glmnet")
install.packages("RcppEigen")
unlink("/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/00LOCK-RcppEigen", recursive = TRUE)
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
library(glmnet)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(glmnet)
# library(glmnet)
ridgefit <- glmnet(X[, -1], y, alpha = 0)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
# g is a function: Rˆn -> R
# grad_g is the gradient of g: a function ∇g: Rˆn -> Rˆn
# hess_g is the hessian of g: a function ∇g: Rˆn -> Rˆ(nxn)
# w0 is the initial point
# K is the number of iterations
newts_method <- function(g, grad_g, hess_g, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point
curr_point <- w0
# create initial empty coordinate matrix: (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# iterate for K iterations
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate hessian at current step
hess_curr <- hess_g(curr_point)
# calculate inverse of the hessian
inv_hess <- solve(hess_curr)
# update step
mat[i, 1:length(w0)] <- curr_point - inv_hess %*% grad_curr
curr_point <- mat[i, 1:length(w0)]
# fill out function output given current point
mat[i, length(w0) + 1] <- g(curr_point)
}
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
# check if there are duplicate minimum values
min_indices <- which(outputs == min_val)
num_min_indices <- length(min_indices)
if (num_min_indices > 1) {
# if there are multiple min values, choose the first one
min_index <- inputs[min_indices[1], ]
} else {
# If there is only one unique minimum value, directly extract its index
min_index <- inputs[min_indices, ]
}
result$index <- min_index
result$val <- min_val
# return result
return(result)
}
# implementing ridge loss function:
n <- nrow(vball_data)
y <- vball_data$spike
ridge_loss_function <- function(w) {
(1/n) * t(y - design_matrix %*% w) %*% (y - design_matrix %*% w) + 1*t(w)%*%w
}
grad_ridge_loss <- function(w) {
(2/n) * t(design_matrix) %*% ((design_matrix %*% w) - y) + 2*1*w
}
hess_ridge_loss <- function(w) {
(2/n) * (t(design_matrix) %*% design_matrix) + 2*1*I
}
# run newton's method...
newts_method(ridge_loss_function, grad_ridge_loss, hess_ridge_loss, c(0, 0, 0, 0), 10)
install.packages("glmnet")
install.packages("RcppEigen")
install.packages("glmnet")
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
ridgefit <- glmnet(X[, -1], y, alpha = 0)
as.matrix(t(coef(ridgefit, s = 1)))
library(glmnet)
ridgefit <- glmnet(X[, -1], y, alpha = 0)
as.matrix(t(coef(ridgefit, s = 1)))
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*I) %*% (t(design_matrix) %*% y)
w_prime
# compute the value of w*:
n <- nrow(vball_data)
y <- vball_data$spike
I <- diag(dim(t(design_matrix)%*%design_matrix)[1])
w_prime <- solve((t(design_matrix)%*%design_matrix) + 1*n*I) %*% (t(design_matrix) %*% y)
w_prime
library(glmnet)
ridgefit <- glmnet(X[, -1], y, alpha = 0)
as.matrix(t(coef(ridgefit, s = 1)))
coefficients <- data.frame(
Intercept = -4.89745,
x_1 = 0.2448018,
x_2 = 0.8635284
)
# Print coefficients table
knitr::kable(coefficients, caption = "Coefficients")
coefficients <- data.frame(
Intercept = -4.89745,
x_1 = 0.2448018,
x_2 = 0.8635284
)
# Print coefficients table
knitr::kable(coefficients, caption = "Coefficients")
