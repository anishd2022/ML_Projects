grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
# implementing normalized gradient descent:
grad_desc_norm <- function(g, h, e, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
g <- function(w) {
w[1]^8 + w[2]^8
}
output_8 <- grad_desc_norm(g, 0.0001, 0.0001, 0.005, c(1.5, 1), 2000)
# implementing normalized gradient descent:
grad_desc_norm <- function(g, h, e, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
g <- function(w) {
w[1]^8 + w[2]^8
}
output_8 <- grad_desc_norm(g, 0.0001, 0.0001, 0.005, c(1.5, 1), 2000)
output_9 <- grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 2000)
# reading in RDS file
gamma_values <- readRDS("gamma_values.rds")
# creating negative log likelihood function:
n <- length(gamma_values)
l <- function(params) {
-(n * (params[1] * log(params[2] - log(gamma(params[1])))) +
(params[1] - 1) * sum(log(gamma_values)) - params[2] * sum(gamma_values))
}
grad_desc_h <- function(g, h, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# update...
mat[i, 1:length(w0)] <- curr_point - alpha * grad_curr
curr_point <- mat[i, 1:length(w0)]
}
# fill out the last column of the matrix:
mat[, length(w0) + 1] <- apply(mat[, 1:length(w0)], 1, function(row) g(row))
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
min_index <- inputs[which(outputs == min_val), ]
print(nrow(result$index))
result$index <- min_index[1, ]
result$val <- min_val
# return result:
result
}
output_7 <- grad_desc_h(l, 0.0001, 0.005, c(1.5, 1), 200)
output_7$index
# implementing normalized gradient descent:
grad_desc_norm <- function(g, h, e, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
g <- function(w) {
w[1]^8 + w[2]^8
}
output_8 <- grad_desc_norm(g, 0.0001, 0.0001, 0.005, c(1.5, 1), 2000)
output_9 <- grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 2000)
# reading in RDS file
gamma_values <- readRDS("gamma_values.rds")
# creating negative log likelihood function:
n <- length(gamma_values)
l <- function(params) {
-(n * (params[1] * log(params[2] - log(gamma(params[1])))) +
(params[1] - 1) * sum(log(gamma_values)) - params[2] * sum(gamma_values))
}
grad_desc_h <- function(g, h, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# update...
mat[i, 1:length(w0)] <- curr_point - alpha * grad_curr
curr_point <- mat[i, 1:length(w0)]
}
# fill out the last column of the matrix:
mat[, length(w0) + 1] <- apply(mat[, 1:length(w0)], 1, function(row) g(row))
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
min_index <- inputs[which(outputs == min_val), ]
print(class(result$index))
result$index <- min_index[1, ]
result$val <- min_val
# return result:
result
}
output_7 <- grad_desc_h(l, 0.0001, 0.005, c(1.5, 1), 200)
output_7$index
# reading in RDS file
gamma_values <- readRDS("gamma_values.rds")
# creating negative log likelihood function:
n <- length(gamma_values)
l <- function(params) {
-(n * (params[1] * log(params[2] - log(gamma(params[1])))) +
(params[1] - 1) * sum(log(gamma_values)) - params[2] * sum(gamma_values))
}
grad_desc_h <- function(g, h, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# update...
mat[i, 1:length(w0)] <- curr_point - alpha * grad_curr
curr_point <- mat[i, 1:length(w0)]
}
# fill out the last column of the matrix:
mat[, length(w0) + 1] <- apply(mat[, 1:length(w0)], 1, function(row) g(row))
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
min_index <- inputs[which(outputs == min_val), ]
print(dim(result$index))
result$index <- min_index[1, ]
result$val <- min_val
# return result:
result
}
output_7 <- grad_desc_h(l, 0.0001, 0.005, c(1.5, 1), 200)
output_7$index
# reading in RDS file
gamma_values <- readRDS("gamma_values.rds")
# creating negative log likelihood function:
n <- length(gamma_values)
l <- function(params) {
-(n * (params[1] * log(params[2] - log(gamma(params[1])))) +
(params[1] - 1) * sum(log(gamma_values)) - params[2] * sum(gamma_values))
}
grad_desc_h <- function(g, h, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# update...
mat[i, 1:length(w0)] <- curr_point - alpha * grad_curr
curr_point <- mat[i, 1:length(w0)]
}
# fill out the last column of the matrix:
mat[, length(w0) + 1] <- apply(mat[, 1:length(w0)], 1, function(row) g(row))
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
# Check if there are duplicate minimum values
min_indices <- which(outputs == min_val)
num_min_indices <- length(min_indices)
if (num_min_indices > 1) {
# If there are multiple minimum values, choose the first one
min_index <- inputs[min_indices[1], ]
} else {
# If there is only one unique minimum value, directly extract its index
min_index <- inputs[min_indices, ]
}
result$index <- min_index[1, ]
result$val <- min_val
# return result:
result
}
output_7 <- grad_desc_h(l, 0.0001, 0.005, c(1.5, 1), 200)
# reading in RDS file
gamma_values <- readRDS("gamma_values.rds")
# creating negative log likelihood function:
n <- length(gamma_values)
l <- function(params) {
-(n * (params[1] * log(params[2] - log(gamma(params[1])))) +
(params[1] - 1) * sum(log(gamma_values)) - params[2] * sum(gamma_values))
}
grad_desc_h <- function(g, h, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# update...
mat[i, 1:length(w0)] <- curr_point - alpha * grad_curr
curr_point <- mat[i, 1:length(w0)]
}
# fill out the last column of the matrix:
mat[, length(w0) + 1] <- apply(mat[, 1:length(w0)], 1, function(row) g(row))
inputs <- mat[, 1:length(w0)]
outputs <- mat[, length(w0) + 1]
min_val <- min(outputs)
# Check if there are duplicate minimum values
min_indices <- which(outputs == min_val)
num_min_indices <- length(min_indices)
if (num_min_indices > 1) {
# If there are multiple minimum values, choose the first one
min_index <- inputs[min_indices[1], ]
} else {
# If there is only one unique minimum value, directly extract its index
min_index <- inputs[min_indices, ]
}
result$index <- min_index
result$val <- min_val
# return result:
result
}
output_7 <- grad_desc_h(l, 0.0001, 0.005, c(1.5, 1), 200)
output_7$index
# implementing normalized gradient descent:
grad_desc_norm <- function(g, h, e, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
g <- function(w) {
w[1]^8 + w[2]^8
}
output_8 <- grad_desc_norm(g, 0.0001, 0.0001, 0.005, c(1.5, 1), 2000)
output_9 <- grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 2000)
# implementing normalized gradient descent:
grad_desc_norm <- function(g, h, e, alpha, w0, K) {
# create initial list object
result <- list(index = c(), val = c())
# set initial point:
curr_point <- w0
# create initial empty coordinate matrix (ncol = n+1)
mat <- matrix(nrow = K, ncol = length(w0) + 1)
# create gradient approximation function:
grad_g <- function(x) {
result_vec <- c()
for (i in 1:length(x)) {
current_result <- (g(x + h) - g(x - h)) / (2*h)
result_vec <- append(result_vec, current_result)
}
result_vec
}
# create gradient normalization function:
get_grad_norm <- function(gradient) {
result <- sqrt(sum(gradient^2))
result
}
# iterate for K iterations...
for (i in 1:K) {
# calculate gradient at current step
grad_curr <- grad_g(curr_point)
# calculate the norm of the gradient:
norm <- get_grad_norm(grad_curr)
# update the current point:
curr_point <- curr_point - ((alpha) / (norm + e)) * grad_curr
mat[i, 1:length(w0)] <- curr_point
mat[i, length(w0) + 1] <- g(curr_point)
}
min_index <- which.min(mat[, length(w0) + 1])
if (length(min_index > 1)) {
min_index <- min_index[1]
}
min_input <- mat[min_index, 1:length(w0)]
min_output <- mat[min_index, length(w0) + 1]
result$index <- min_input
result$val <- min_output
# return result:
result
}
g <- function(w) {
w[1]^8 + w[2]^8
}
output_8 <- grad_desc_norm(g, 0.0001, 0.0001, 0.005, c(1.5, 1), 2000)
output_9 <- grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 2000)
output_8
output_9
grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 4000)
grad_desc_h(g, 0.0001, 0.005, c(1.5, 1), 10000)
